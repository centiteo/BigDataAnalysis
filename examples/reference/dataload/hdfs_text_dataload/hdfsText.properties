# define mode
dataload.mode=local
#dataload.source.streaming.fetch=false

# define data source
dataload.source.type=hdfs
dataload.source.hdfsDirs=/user/root/testOutput
# define data format
dataload.source.parserType=text
dataload.source.instanceDocPath=/Users/centiteo/GitHub/BigDataAnalysis/examples/reference/dataload/hdfs_text_dataload/txtRecord.xml

# define client for mapper
dataload.client.queueLength=500
dataload.client.fetchParallel=1
dataload.client.threadsPerMapper=1

# define target table
hbase.target.table.name=text_test
#hbase.table.splitKeyPrefixes=16001,16009,16026,16027,16030,16089,16098,16118,16182
hbase.table.splitSize=18
hbase.table.writeToWAL=false
hbase.table.writeBufferSize=6
hbase.table.autoFlush=false
hbase.table.memStore=64

# index definition
#buildIndex=true
#indexConfFileName=test_index-conf.xml
#server.search.enableStatistics=false
#useHashUUIDForRowkey=true
#regionQuantity=60
#hbaseCoprocessorLocation=hdfs://vt-nn:8020/user/asb/BigDataAnalysis-1.0.jar
